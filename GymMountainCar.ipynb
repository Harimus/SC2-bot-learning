{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import sys\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v0')\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "# observation = Box(2)  with 0: position [-1.2,0.6] 1: velocity [-0.07, 0.07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "class RLAgent():\n",
    "    def __init__(self,learning_rate,state_size,\n",
    "                 action_size,hidden_size,epsilon = 0.5, \n",
    "                 gamma = 0.7,epsilon_min = 0.01, \n",
    "                 epsilon_decay = 0.99,batch_size = 32,\n",
    "                 decay = 0.01):\n",
    "        #RL parameters\n",
    "        self.memory = deque(maxlen = 100000)\n",
    "        self.state_size = state_size\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        #epsilon learning param\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(layers.Dense(hidden_size, input_dim=state_size, activation = 'tanh'))\n",
    "        self.model.add(layers.Dense(hidden_size, activation = 'tanh'))\n",
    "        self.model.add(layers.Dense(action_size,activation = 'linear'))\n",
    "        self.model.compile(loss = 'mse',\n",
    "                            optimizer = Adam()) #potential improvement in AdamOpt          \n",
    "        \n",
    "        \n",
    "    def action(self,state):\n",
    "        if(np.random.random() <= self.epsilon):\n",
    "          return np.random.randint(self.action_size)\n",
    "\n",
    "        return np.argmax(self.model.predict(state))\n",
    "        \n",
    "                  \n",
    "    def memorize(self, state, action, reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    def forget(self):\n",
    "        self.memory.clear()\n",
    "            \n",
    "    def learn(self, batch_size = None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "        x_batch, y_batch = [],[]\n",
    "        batch = random.sample(self.memory, min(len(self.memory),batch_size))\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "          y_target = self.model.predict(state)\n",
    "          y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "          x_batch.append(state[0])\n",
    "          y_batch.append(y_target[0])\n",
    "        self.model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose = 0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "          self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "gamma = 0.99\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0\n",
      "41.45\n",
      "105.68\n",
      "191.0\n",
      "192.34\n",
      "181.86\n",
      "187.63\n",
      "188.5\n",
      "179.98\n",
      "191.57\n",
      "180.43\n",
      "179.74\n",
      "176.15\n",
      "185.52\n",
      "190.77\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "Agent = RLAgent(1e-5,observation_size,action_size,24)\n",
    "init = tf.global_variables_initializer()\n",
    "#\n",
    "total_episodes = 1500\n",
    "max_ep = 201 #is set as 200 in Mountain Car, unchangeable.\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "s = env.reset()\n",
    "total_reward = []\n",
    "total_length = []\n",
    "done_once = False\n",
    "i = 0\n",
    "while i < total_episodes:\n",
    "    s = env.reset()\n",
    "    running_reward = 0\n",
    "    d = False\n",
    "    for j in range(max_ep):\n",
    "        #Note that the agent assumes batch input of state, so state have to be np array[ [state_values]]\n",
    "        a = Agent.action(s.reshape(1,-1))                \n",
    "        s1,r,d,_ = env.step(a)  #new state, reward, done\n",
    "        Agent.memorize(s.reshape(1,-1),a,r,s1.reshape(1,-1),d)\n",
    "        running_reward += r\n",
    "        if  i % 100 == 0:# and (total_episodes - i) < 500:\n",
    "            env.render()\n",
    "           # pdb.set_trace()\n",
    "        s = s1\n",
    "        if d:\n",
    "            Agent.learn(j)\n",
    "            break\n",
    "    total_reward.append(running_reward)\n",
    "    Agent.forget()\n",
    "    if i % 100 == 0:\n",
    "        print(str(np.mean(total_reward[-100:])) + ' ' + str(i//100) + '/'+str(total_episodes//100) )       \n",
    "    i += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward:200.0\n"
     ]
    }
   ],
   "source": [
    "#final run of learned agent\n",
    "s = env.reset()\n",
    "a = Agent.action(s.reshape(1,-1))\n",
    "s1 , r , d , _ =env.step(a)\n",
    "total_reward = r\n",
    "while not d:\n",
    "    s = s1\n",
    "    a = Agent.action(s.reshape(1,-1))\n",
    "    s1, r, d, _ = env.step(a)\n",
    "    total_reward += r\n",
    "    env.render()\n",
    "print(\"total reward:\" + str(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
