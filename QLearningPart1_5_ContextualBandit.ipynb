{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandits\n",
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #lower number, the better. Therefore optimal [4,2,1]\n",
    "        self.bandits = np.array([ [0.2, 0 ,-0.0, -5 ], [0.1, -5,1,0.25],[-5,5,5,5] ])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "    \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0,len(self.bandits))\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self,action):\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "class agent():\n",
    "    def __init__(self, lr , s_size, a_size):\n",
    "        \n",
    "        self.state_in = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in,s_size)\n",
    "        \n",
    "        output = slim.fully_connected(state_in_OH,a_size,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn= tf.nn.sigmoid,\n",
    "                                      weights_initializer = tf.ones_initializer())\n",
    "            \n",
    "        self.output = tf.reshape(output,[-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        self.reward_holder = tf.placeholder( shape=[1], dtype = tf.float32)\n",
    "        \n",
    "        self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        \n",
    "        self.responsible_weight = tf.slice(self.output,self.action_holder,[1])\n",
    "        \n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        \n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 3 bandits: \n",
      " [[-1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -1.  -2.  -4. 160.]\n",
      " [ -2. 151.  -3.   0.]\n",
      " [165.  -2.  -2.  -6.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -2.   0.  -1. 325.]\n",
      " [ -3. 319.  -6.  -1.]\n",
      " [308.  -4.  -7.  -7.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -3.   5.   0. 487.]\n",
      " [ -2. 475. -10.  -3.]\n",
      " [460.  -5. -10. -12.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -5.   7.   0. 651.]\n",
      " [ -1. 629. -14.  -6.]\n",
      " [613. -11. -14. -14.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -5.  11.   2. 818.]\n",
      " [ -2. 773. -18.  -8.]\n",
      " [773. -15. -20. -15.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[ -5.  13.   3. 978.]\n",
      " [ -4. 915. -23. -10.]\n",
      " [937. -16. -30. -19.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-6.000e+00  1.100e+01  3.000e+00  1.144e+03]\n",
      " [-1.000e+00  1.069e+03 -2.500e+01 -1.100e+01]\n",
      " [ 1.097e+03 -1.900e+01 -3.600e+01 -2.200e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[  -8.   10.    2. 1296.]\n",
      " [   0. 1231.  -25.  -13.]\n",
      " [1258.  -22.  -39.  -27.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[  -8.   10.   -2. 1434.]\n",
      " [  -2. 1395.  -28.  -16.]\n",
      " [1427.  -25.  -41.  -34.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-4.000e+00  8.000e+00 -1.000e+00  1.571e+03]\n",
      " [ 1.000e+00  1.569e+03 -2.800e+01 -1.800e+01]\n",
      " [ 1.590e+03 -2.700e+01 -4.400e+01 -3.800e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-4.000e+00  6.000e+00 -1.000e+00  1.739e+03]\n",
      " [ 2.000e+00  1.725e+03 -3.100e+01 -2.100e+01]\n",
      " [ 1.743e+03 -2.900e+01 -5.000e+01 -4.500e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-5.000e+00  3.000e+00 -4.000e+00  1.894e+03]\n",
      " [ 1.000e+00  1.887e+03 -3.600e+01 -2.300e+01]\n",
      " [ 1.904e+03 -3.000e+01 -5.300e+01 -4.900e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-9.000e+00  3.000e+00 -2.000e+00  2.043e+03]\n",
      " [ 4.000e+00  2.046e+03 -4.200e+01 -2.300e+01]\n",
      " [ 2.069e+03 -3.600e+01 -5.600e+01 -5.300e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-8.000e+00  4.000e+00 -2.000e+00  2.196e+03]\n",
      " [ 0.000e+00  2.205e+03 -4.300e+01 -2.400e+01]\n",
      " [ 2.232e+03 -3.900e+01 -5.800e+01 -5.800e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-8.000e+00  3.000e+00 -3.000e+00  2.368e+03]\n",
      " [ 1.000e+00  2.360e+03 -4.700e+01 -3.000e+01]\n",
      " [ 2.370e+03 -4.500e+01 -6.100e+01 -6.200e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[-5.000e+00  6.000e+00  2.000e+00  2.494e+03]\n",
      " [ 2.000e+00  2.516e+03 -5.000e+01 -3.100e+01]\n",
      " [ 2.549e+03 -5.100e+01 -6.900e+01 -6.600e+01]]\n",
      "Running reward for the 3 bandits: \n",
      " [[  -8.    6.    4. 2670.]\n",
      " [   4. 2675.  -51.  -33.]\n",
      " [2687.  -54.  -72.  -74.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[  -5.    4.    5. 2835.]\n",
      " [   3. 2829.  -56.  -33.]\n",
      " [2837.  -62.  -76.  -78.]]\n",
      "Running reward for the 3 bandits: \n",
      " [[  -6.    5.    3. 2994.]\n",
      " [   0. 2985.  -59.  -34.]\n",
      " [2991.  -67.  -80.  -86.]]\n",
      "The agent thinks action 4for bandit 1 is the most promising....\n",
      "and it was right!\n",
      "The agent thinks action 2for bandit 2 is the most promising....\n",
      "and it was right!\n",
      "The agent thinks action 1for bandit 3 is the most promising....\n",
      "and it was right!\n",
      "fully_connected/weights:0 [[0.99838704 1.0010782  1.0008087  1.6420072 ]\n",
      " [1.0002705  1.6436281  0.98349786 0.9908272 ]\n",
      " [1.6420072  0.98186314 0.97722286 0.9758551 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cBandit = contextual_bandit()\n",
    "\n",
    "myAgent = agent(lr=0.001, s_size = cBandit.num_bandits, a_size=cBandit.num_actions)\n",
    "\n",
    "weights = tf.trainable_variables()[0]\n",
    "total_episodes = 10000\n",
    "\n",
    "\n",
    "total_reward = np.zeros([cBandit.num_bandits,cBandit.num_actions])\n",
    "e = 0.1\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit()\n",
    "        \n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[s]})\n",
    "            \n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        #update network\n",
    "        _,ww = sess.run([myAgent.update, weights]\n",
    "                              , feed_dict={myAgent.reward_holder:[reward],myAgent.action_holder:[action],myAgent.state_in:[s]})\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 509 == 0:\n",
    "            print(\"Running reward for the \" + str(cBandit.num_bandits) + \n",
    "                  \" bandits: \\n \" + str(total_reward))\n",
    "        i+=1\n",
    "    for a in range(cBandit.num_bandits):    \n",
    "        print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \"for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "        if np.argmax(ww[a]) == np.argmax(-np.array(cBandit.bandits[a])):\n",
    "            print( \"and it was right!\")\n",
    "        else:\n",
    "            print(\"Wrong!\")\n",
    "            \n",
    "    trained_weights = tf.trainable_variables()\n",
    "    twout = sess.run(trained_weights)          \n",
    "    for var, val in zip(trained_weights,twout):\n",
    "        print(var.name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ef78287b5af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtwout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OpenAIGym/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OpenAIGym/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "\n",
    "#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
